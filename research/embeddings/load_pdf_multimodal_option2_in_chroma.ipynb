{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3b635f39ca3868",
   "metadata": {},
   "source": [
    "## Semi-structured and Multi-modal RAG\n",
    "\n",
    "Many documents contain a mixture of content types, including text, tables, and images.\n",
    "\n",
    "Semi-structured data can be challenging for conventional RAG for at least two reasons:\n",
    "\n",
    "* Text splitting may break up tables, corrupting the data in retrieval\n",
    "* Embedding tables may pose challenges for semantic similarity search\n",
    "\n",
    "And the information captured in images is typically lost.\n",
    "\n",
    "In this notebook we will try option 2.\n",
    "\n",
    "`Option 2:`\n",
    "\n",
    "* Use a multimodal LLM (such as [GPT4-V](https://openai.com/research/gpt-4v-system-card), [LLaVA](https://llava.hliu.cc/), or [FUYU-8b](https://www.adept.ai/blog/fuyu-8b)) to produce text summaries from images\n",
    "* Embed and retrieve text\n",
    "* Pass text chunks to an LLM for answer synthesis\n",
    "\n",
    "This notebook shows how we might tackle this :\n",
    "\n",
    "- We will use Unstructured to parse images, text, and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever to store raw tables, text, (optionally) images along with their summaries for retrieval.$\n",
    "\n",
    "![ss_mm_rag.png](../../diagrams/ss_mm_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:55:15.239674Z",
     "start_time": "2024-11-18T22:55:01.545379Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\pythonenvironments\\IP2AICHAT_3_11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "load_dotenv(\"../../.env.research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfe171e415ee88f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:56:21.788650Z",
     "start_time": "2024-11-18T22:55:15.271106Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.getenv(\"DATA_DIR\") + \"/rummikub_rules_with_images.pdf\",\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=2500,\n",
    "    new_after_n_chars=2200,\n",
    "    combine_text_under_n_chars=1000,\n",
    "    image_output_dir_path=os.getenv(\"DATA_DIR\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f581de1c0cbfe34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:56:22.215999Z",
     "start_time": "2024-11-18T22:56:22.211443Z"
    }
   },
   "outputs": [],
   "source": [
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        tables.append(str(element))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        texts.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d48f810bd3e142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:56:22.921418Z",
     "start_time": "2024-11-18T22:56:22.221511Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7ef234ed2ab6d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:57:57.470029Z",
     "start_time": "2024-11-18T22:56:22.946912Z"
    }
   },
   "outputs": [],
   "source": [
    "tables = [i for i in tables]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922435d56aeb5c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:57:57.542096Z",
     "start_time": "2024-11-18T22:57:57.533576Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_and_encode_image(image_path, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image and encode it as Base64.\n",
    "\n",
    "    Args:\n",
    "    image_path (str): Path to the image.\n",
    "    size (tuple): Desired size of the image (width, height).\n",
    "\n",
    "    Returns:\n",
    "    str: Base64 string of the resized image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open and resize the image\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"RGB\")  # Ensure consistent format\n",
    "            img.thumbnail(size, Resampling.LANCZOS)\n",
    "\n",
    "            # Save resized image to a bytes buffer\n",
    "            buffered = io.BytesIO()\n",
    "            img.save(buffered, format=\"PNG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3755a230c68efb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:57:58.373255Z",
     "start_time": "2024-11-18T22:57:57.597827Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_image = \"\"\"You are an assistant that can interpret images and text. The images are all from rules about the game rummikub.\n",
    "Here is an image in Base64 format:\n",
    "{element}\n",
    "\n",
    "Please describe this image in detail.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_image)\n",
    "\n",
    "multi_model = ChatOllama(model=\"llava-llama3\")\n",
    "\n",
    "image_summarize_chain = (\n",
    "    {\"element\": lambda x: resize_and_encode_image(x)}\n",
    "    | prompt\n",
    "    | multi_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12681c12a7b9101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T23:00:18.027138400Z",
     "start_time": "2024-11-18T22:57:58.378032Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image_dir = os.getenv(\"IMG_DIR\")\n",
    "image_paths = [\n",
    "    os.path.join(image_dir, img)\n",
    "    for img in os.listdir(image_dir)\n",
    "    if img.endswith((\".jpg\", \".png\"))\n",
    "]\n",
    "\n",
    "image_summaries = image_summarize_chain.batch(image_paths, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b1aacaa834566",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path, summary in zip(image_paths, image_summaries):\n",
    "    print(f\"Afbeelding: {img_path}\\nBeschrijving: {summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37756d0d80b8d6a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Option 2 is not possible and option 3 will neither be possible because even after resizing to try and reduce the size of the images, it still takes too long to make a description of the images. I think it is because the local model is not powerful enough to handle the images or the images are too complex to describe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
